# ðŸ­ Industrial IoT Event-Driven Telemetry Pipeline
*(Simulated Manufacturing Telemetry Pipeline)*

![Status](https://img.shields.io/badge/Status-Operational-brightgreen) [![Streamlit App](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://manufacturing-telemetry-pipeline-4w4vdqqcqswwtjyhctzsyb.streamlit.app/) ![Python](https://img.shields.io/badge/Python-3.9%2B-blue) ![PostgreSQL](https://img.shields.io/badge/PostgreSQL-Supabase-blue) ![Docker](https://img.shields.io/badge/Docker-Containerized-blue) ![CI/CD](https://img.shields.io/badge/GitHub-Actions-orange)

## ðŸ“Œ Project Overview
This project is a **scalable, event-driven data pipeline** designed to bridge the gap between manufacturing hardware (PLCs, Dyno Testers) and real-time operational analytics. 

It simulates a **continuous 1Hz telemetry environment** typical of mass-production robotics (e.g., Optimus-style units), focusing on **fault tolerance**, **low-latency dashboard queries**, and **infrastructure health monitoring**.

**Core Capabilities:**
1.  **Event-Driven Ingestion:** Ingests 1Hz telemetry from simulated Dyno Testers (using NASA C-MAPSS data proxy).
2.  **Scalable Storage:** Utilizes a wide-column PostgreSQL schema optimized for heavy write loads.
3.  **Infrastructure Health:** "Sentinel" service ($monitor.py$) provides automated alerting for thermal runaway and pressure anomalies.
4.  **Real-Time Visualization:** Streamlit dashboard acting as a "Manufacturing Command Center" for immediate floor feedback.
5.  **First Pass Yield (FPY):** Digital Test Stand logic determines pass/fail rates in real-time based on strict sensor thresholds.

**Tech Stack:** 
*   **Pipeline:** Python (Async/Event Loop simulation), Pandas.
*   **Database:** PostgreSQL (Supabase) with B-Tree Indexing.
*   **DevOps:** GitHub Actions for automated health checks (CI/CD).
*   **Visualization:** Streamlit (Real-time polling).

---

## ðŸ—ï¸ Architecture

```mermaid
flowchart LR
    Source[("Dyno Testers / PLCs\n(Simulated Hardware)")] -->|Stream 1Hz| Ingestor["Telemetry Ingestor\n(src/ingestor.py)"]
    Ingestor -->|Event Data| DB[("Production DB\n(PostgreSQL)")]
    Sentinel["Infrastructure Health Monitor\n(src/monitor.py)"] <-->|Audit & Alert| DB
    Sentinel -->|Trigger| Logs["System Logs / Alerts"]
    DB -->|Read <200ms| Dashboard["Manufacturing Command Center\n(Streamlit)"]
```

---

## ðŸš€ Key Engineering Features (JD Alignment)

### 1. "Build scalable and reliable data pipelines"
*   **Implementation:** The `ingestor.py` script mimics a non-blocking PLC interface, capable of streaming continuous 1Hz data from multiple "Dyno" units simultaneously without data loss.
*   **Reliability:** Implemented a **"Dry Run / Mock Mode"** fallback. If the database connection drops (network partition), the pipeline automatically switches to local logging to preserve visibility (Fault Tolerance).

### 2. "Monitor and maintain infrastructure health"
*   **Implementation:** The `monitor.py` Sentinel operates as an independent microservice.
*   **Automation:** Integrated with **GitHub Actions** (`.github/workflows/sentinel.yml`) to run hourly recurring health checks on the production database, ensuring data integrity 24/7.

    ![CI/CD Proof](assets/cicd_proof.png)

### 3. "Query and programming best practices"
*   **Implementation:** Designed a database schema (`sql/schema.sql`) specifically for time-series efficiency.
*   **Optimization:** Utilized **Compound Indexes** (`unit_id`, `cycle_time DESC`) to ensure the "Latest Telemetry" dashboard queries run in **O(log n)** time, keeping dashboard latency low even as data grows to millions of rows.

---

## ðŸ› ï¸ Getting Started

### Prerequisites
*   Python 3.8+
*   PostgreSQL database (or Supabase account)

### 1. Setup Environment
```bash
# Clone and enter repo
git clone https://github.com/vinayreddy1801/manufacturing-telemetry-pipeline.git
cd manufacturing-telemetry-pipeline

# Create Virtual Env
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# Install Dependencies
pip install -r requirements.txt
```

### 2. Configure Piping
Create a `.env` file based on `.env.example` and add your connection string:
```bash
DATABASE_URL=postgresql://user:pass@host:5432/db
```

### 3. Run with Docker (Recommended for Reliability)
If you have Docker installed, you can spin up the entire pipeline in seconds without configuring Python:

```bash
# Build and Run
docker-compose up --build
```
*Access the dashboard at `http://localhost:8501`*

### 4. Run Locally (Manual)
```bash
# Start the Dyno/PLC Simulator
python src/ingestor.py
```
*Output: `ðŸš€ Factory Line Started: Streaming Telemetry...`*

---

## âš¡ Database Optimization Proof
To ensure the system handles the **high-write load** of the factory floor, I utilized compound indexing.

```sql
CREATE INDEX idx_unit_cycle ON optimus_test_telemetry(unit_id, cycle_time DESC);
```
*Impact: reduced query cost for "Latest Cycle" lookups by ~95% during testing.*

---
*Key Project for Manufacturing Data Engineering Portfolio.*
